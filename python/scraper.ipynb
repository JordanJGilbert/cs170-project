{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.141025641025641\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from starter import *\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://170-leaderboard.vercel.app/team/discordggTYsy64VcWT\"\n",
    "page = requests.get(url)\n",
    "data = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "def get_team_json_table():\n",
    "    json_table = data.find_all('script')[9].string\n",
    "    json_table = json.loads(json_table)\n",
    "    json_table = json_table[\"props\"][\"pageProps\"][\"data\"]\n",
    "    return json_table\n",
    "\n",
    "def get_result_json_table():\n",
    "    json_table = data.find_all('script')[9].string\n",
    "    json_table = json.loads(json_table)\n",
    "    json_table = json_table[\"props\"][\"pageProps\"][\"sortedData\"]\n",
    "    return json_table\n",
    "\n",
    "def expand_input(name):\n",
    "    if name.startswith('large'):\n",
    "        return ('large', int(name[5:-3]))\n",
    "    elif name.startswith('medium'):\n",
    "        return ('medium', int(name[6:-3]))\n",
    "    elif name.startswith('small'):\n",
    "        return ('small', int(name[5:-3]))\n",
    "\n",
    "class Result:\n",
    "    size = None\n",
    "    test = None\n",
    "    rank = None\n",
    "    true_rank = None\n",
    "    best_score = None\n",
    "    submission_score = None\n",
    "    local_score = None\n",
    "    local_file = None\n",
    "    url = None\n",
    "    notes = \"_\"\n",
    "\n",
    "    def __init__(self, name, rank, best_score, submission_score):\n",
    "        self.size, self.test = expand_input(name)\n",
    "        self.rank = rank\n",
    "        self.true_rank = rank\n",
    "        self.best_score = best_score\n",
    "        self.submission_score = submission_score\n",
    "        self.url = f\"https://170-leaderboard.vercel.app/input/{self.size}/{self.test}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.size} {self.test} {self.rank} {self.best_score} {self.submission_score} {self.local_score} {self.local_file} {self.url} {self.notes}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "with open(\"../queue.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "team_json_table = get_team_json_table()\n",
    "table = dict()\n",
    "for key in team_json_table:\n",
    "    table[key] = (team_json_table[key][\"score\"], team_json_table[key][\"rank\"])\n",
    "\n",
    "sorted_table = []\n",
    "for key in table:\n",
    "    sorted_table.append(Result(key, table[key][1], table[key][0], table[key][0]))\n",
    "sorted_table.sort(key=lambda x: x.rank, reverse=True)\n",
    "\n",
    "for result in sorted_table:\n",
    "    if result.rank != 1:\n",
    "        page = requests.get(result.url)\n",
    "        data = BeautifulSoup(page.content, 'html.parser')\n",
    "        result_json_table = get_result_json_table()\n",
    "        result.best_score = result_json_table[0][\"score\"]\n",
    "\n",
    "best_locals = dict()\n",
    "\n",
    "for size in ['small', 'medium', 'large']:\n",
    "    for test in range(1, 261):\n",
    "        best_file = 'inf.in'\n",
    "        best_score = float('inf')\n",
    "        folder = '../tests/' + size + '/' + size + str(test)\n",
    "        G = read_input(folder + '/graph.in')\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith('.out') and float((filename.split('_')[0]).split('.')[0]) <= \\\n",
    "                    float((best_file.split('_')[0]).split('.')[0]):\n",
    "                read_output(G, folder + '/' + filename)\n",
    "                validate_output(G)\n",
    "                score_G = score(G)\n",
    "                if round(score_G) != float((filename.split('_')[0]).split('.')[0]):\n",
    "                    print('Score mismatch, file: ' + folder + '/' + filename + ', score: ' + str(score_G))\n",
    "                if score_G < best_score:\n",
    "                    best_score = score_G\n",
    "                    best_file = filename\n",
    "        best_locals[(size, str(test))] = (best_score, best_file[:-4])\n",
    "        \n",
    "for result in sorted_table:\n",
    "    result.local_score, result.local_file = best_locals[(result.size, str(result.test))]\n",
    "    if result.submission_score == result.best_score:\n",
    "        result.notes = \"published_rank_1\"\n",
    "    elif result.local_score <= result.best_score:\n",
    "        result.true_rank = 1\n",
    "        result.notes = \"sleeper\"\n",
    "    elif result.local_score > result.submission_score:\n",
    "        result.notes = \"missing_local\"\n",
    "\n",
    "sorted_table.sort(key=lambda x: min(x.submission_score, x.local_score) - x.best_score, reverse=True)\n",
    "\n",
    "for result in sorted_table:\n",
    "    print(result, min(result.submission_score, result.local_score) - result.best_score, file=open(\"../queue.txt\", \"a\"))\n",
    "\n",
    "true_avg_rank = 0\n",
    "for result in sorted_table:\n",
    "    true_avg_rank += result.true_rank\n",
    "true_avg_rank /= len(sorted_table)\n",
    "print(true_avg_rank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
